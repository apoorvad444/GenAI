# Course Material Q&A Assistant Project

<!-- PROJECT LOGO -->
<p align="center">
  <img src="https://raw.githubusercontent.com/simple-icons/simple-icons/develop/icons/streamlit.svg" alt="Logo" width="90" height="90">
</p>

<h1 align="center">ğŸ“š PDF Q&A Assistant</h1>

<p align="center">
  <i>Ask intelligent, context-aware questions from your PDFs using Retrieval-Augmented Generation (RAG)</i>  
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue.svg?style=for-the-badge&logo=python" alt="Python">
  <img src="https://img.shields.io/badge/Streamlit-1.30+-red.svg?style=for-the-badge&logo=streamlit" alt="Streamlit">
  <img src="https://img.shields.io/badge/LangChain-âœ¨-purple.svg?style=for-the-badge&logo=chainlink" alt="LangChain">
  <img src="https://img.shields.io/badge/HuggingFace-ğŸ¤—-yellow.svg?style=for-the-badge&logo=huggingface" alt="Hugging Face">
  <img src="https://img.shields.io/badge/FAISS-Facebook-blue.svg?style=for-the-badge&logo=facebook" alt="FAISS">
</p>

<p align="center">
  <a href="#-features">Features</a> â€¢
  <a href="#-tech-stack">Tech Stack</a> â€¢
  <a href="#-installation">Installation</a> â€¢
  <a href="#-usage">Usage</a> â€¢
  <a href="#-deployment">Deployment</a> â€¢
  <a href="#-screenshots">Screenshots</a> â€¢
  <a href="#-license">License</a>
</p>

---

## ğŸš€ Why This Project?

Large PDF documents (textbooks, research papers, manuals) are **hard to navigate**. Traditional keyword search (`CTRL+F`) fails when queries are phrased differently.  

This app solves that by combining **semantic vector search** with **LLMs** â†’ giving you **contextual, accurate answers** instead of keyword matches.  

âœ¨ **Core Idea:**  
- Use **RAG (Retrieval-Augmented Generation)** â†’ retrieval + LLM reasoning  
- Store embeddings in **FAISS** for semantic search  
- Generate answers with **Falcon-7B-Instruct** or **GPT-Neo-125M**  
- Smooth **Streamlit UI** for an end-user friendly interface  

---

## âœ¨ Features

- ğŸ“‚ Upload **multiple PDFs** at once  
- ğŸ” Automatic **text chunking + embeddings**  
- ğŸ’¾ Persistent **FAISS index** (reset anytime)  
- ğŸ§  **LLM-powered Q&A** (Falcon-7B or GPT-Neo-125M)  
- ğŸ“ **Q&A history** maintained across a session  
- ğŸ›ï¸ Model switcher:  
  - **Falcon-7B-Instruct** â†’ powerful, accurate answers  
  - **GPT-Neo-125M** â†’ lightweight fallback for free-tier CPU  

---

## ğŸ›  Tech Stack

| Component | Technology |
|-----------|------------|
| **Frontend** | [Streamlit](https://streamlit.io/) |
| **Orchestration** | [LangChain](https://www.langchain.com/) |
| **Vector Store** | [FAISS](https://github.com/facebookresearch/faiss) |
| **Embeddings** | [SentenceTransformers](https://www.sbert.net/) (`all-MiniLM-L6-v2`) |
| **LLMs** | [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct), [GPT-Neo-125M](https://huggingface.co/EleutherAI/gpt-neo-125M) |
| **Deep Learning** | [PyTorch](https://pytorch.org/) |
| **PDF Parsing** | [PyPDF2](https://pypi.org/project/PyPDF2/) |

---

## ğŸ“‚ Project Structure

```

.
â”œâ”€â”€ app.py              # Main Streamlit application
â”œâ”€â”€ requirements.txt    # High-level dependencies
â”œâ”€â”€ constraints.txt     # Pinned versions for reproducibility
â”œâ”€â”€ README.md           # Documentation
â””â”€â”€ uploaded_pdfs/      # User-uploaded PDFs

````

---

## ğŸ”§ Installation

Clone the repo and set up a virtual environment:

```bash
git clone https://github.com/apoorvad444/GenAI.git
cd pdf-qa-assistant

python -m venv venv
source venv/bin/activate   # macOS/Linux
venv\Scripts\activate      # Windows

pip install -r requirements.txt -c constraints.txt
````

---

## â–¶ï¸ Usage

Run the app:

```bash
streamlit run app.py
```

Then open **[http://localhost:8501](http://localhost:8501)** in your browser.

---

## ğŸš€ Deployment

### ğŸ”´ Streamlit Cloud

1. Push repo to GitHub
2. Deploy on **Streamlit Cloud**
3. âš ï¸ Use **GPT-Neo-125M** for free CPU tier (Falcon-7B may exceed memory)

### ğŸ¤— Hugging Face Spaces

1. Create a new **Streamlit Space**
2. Select a **GPU runtime** (T4/A10/A100 recommended)
3. Push this repo â†’ Falcon-7B works smoothly

---


### ğŸ“‘ Example Workflow

1. Upload your course material PDFs
2. App extracts & chunks text
3. FAISS builds a vector index
4. Ask questions â†’ relevant chunks retrieved â†’ LLM generates contextual answers

![Workflow Screenshot](https://via.placeholder.com/800x400?text=Workflow)



## ğŸ“œ License

Distributed under the **MIT License**.
See [LICENSE](./LICENSE) for details.



## ğŸ‘¨â€ğŸ’» Author

Developed with â¤ï¸ by **[Abhinav Jajoo,
Apporva Dixit,Vrinda Sharma]**


## ğŸ™Œ Acknowledgements

* [Falcon LLM](https://huggingface.co/tiiuae/falcon-7b-instruct)
* [Hugging Face Transformers](https://huggingface.co/transformers/)
* [LangChain](https://www.langchain.com/)
* [Streamlit](https://streamlit.io/)
* [SBERT](https://www.sbert.net/)

```

